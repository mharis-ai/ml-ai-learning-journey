{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07eb9992",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\"><strong>Week 3: Classification</strong></h2>\n",
    "\n",
    "- Introduction to Classification\n",
    "- Logistic Regression\n",
    "- Decision Boundary\n",
    "- Cost Function for Logistic Regression\n",
    "- Gradient Descent Implementation for Logistic Regression\n",
    "- Overfitting\n",
    "- Regularization in Machine Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c04e1",
   "metadata": {},
   "source": [
    "## **Introduction to Classification**\n",
    "\n",
    "Classification is a **supervised learning task** where the output variable $y$ can take on only **a limited set of discrete values**, rather than any value in a continuous range.  \n",
    "\n",
    "In binary classification, $y$ takes only **two possible values**, typically $0$ or $1$.  \n",
    "\n",
    "Examples of binary classification problems include:\n",
    "\n",
    "- Predicting whether an email is **spam ($1$)** or **not spam ($0$)**  \n",
    "- Determining if an online transaction is **fraudulent ($1$)** or **legitimate ($0$)**  \n",
    "- Diagnosing a tumor as **malignant ($1$)** or **benign ($0$)**  \n",
    "\n",
    "### **Positive and Negative Classes**\n",
    "\n",
    "By convention:\n",
    "\n",
    "- The class corresponding to $y = 1$ is called the **positive class**  \n",
    "- The class corresponding to $y = 0$ is called the **negative class**  \n",
    "\n",
    "For example:\n",
    "\n",
    "| Problem                  | Positive Class ($y=1$) | Negative Class ($y=0$) |\n",
    "|--------------------------|------------------------|------------------------|\n",
    "| Email spam detection      | Spam                   | Not Spam               |\n",
    "| Transaction fraud detection | Fraudulent           | Legitimate             |\n",
    "| Tumor diagnosis          | Malignant              | Benign                 |\n",
    "\n",
    "*Note: Positive and negative do **not imply good or bad**. They simply indicate the presence or absence of a particular property.*\n",
    "\n",
    "### **Why Linear Regression is Not Suitable for Classification**\n",
    "\n",
    "Linear regression predicts a **continuous output**, which can take any real value.  \n",
    "\n",
    "For classification, this causes several issues:\n",
    "\n",
    "- Predictions can be **less than $0$** or **greater than $1$**, which are not valid probabilities  \n",
    "- A simple threshold (e.g., $0.5$) can be applied, but the model can be **highly sensitive to outliers**  \n",
    "- Adding a single extreme data point can **shift the decision boundary** dramatically, reducing reliability\n",
    "\n",
    "### **Need for Logistic Regression**\n",
    "\n",
    "Logistic regression addresses these limitations by:\n",
    "\n",
    "- Constraining outputs to the range **$[0, 1]$**  \n",
    "- Providing **probabilistic interpretation** of predictions  \n",
    "- Maintaining **stable decision boundaries** even with new or extreme data points  \n",
    "\n",
    "This makes logistic regression a **preferred method for binary classification problems**, forming the foundation for more advanced classification techniques.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf694a7f",
   "metadata": {},
   "source": [
    "## **Logistic Regression**\n",
    "\n",
    "Logistic Regression is a **classification algorithm** used for problems where the output label $y$ can take only **two values**, typically $0$ or $1$. In applications such as tumor classification, the goal is to predict whether a tumor is **benign (0)** or **malignant (1)**.\n",
    "\n",
    "Linear regression is not suitable for this type of problem because its predictions are unbounded and cannot be interpreted as probabilities. Logistic regression addresses this limitation by fitting an **S-shaped curve** that outputs values strictly between **0 and 1**.\n",
    "\n",
    "### **Sigmoid (Logistic) Function**\n",
    "\n",
    "A key component of logistic regression is the **sigmoid function**, also known as the **logistic function**. This function maps any real-valued number to a value between 0 and 1.\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $z$ is any real number  \n",
    "- $e$ is a mathematical constant approximately equal to $2.718$\n",
    "\n",
    "### **Behavior of the Sigmoid Function**\n",
    "\n",
    "The sigmoid function has a characteristic S-shape:\n",
    "\n",
    "- When $z$ is a **large positive number**, $e^{-z}$ becomes very small and $g(z)$ approaches **1**\n",
    "- When $z$ is a **large negative number**, $e^{-z}$ becomes very large and $g(z)$ approaches **0**\n",
    "- When $z = 0$:\n",
    "\n",
    "$$\n",
    "g(0) = \\frac{1}{1 + 1} = 0.5\n",
    "$$\n",
    "\n",
    "This is why the sigmoid curve crosses the vertical axis at **0.5**.\n",
    "\n",
    "### **Logistic Regression Model**\n",
    "\n",
    "Logistic regression is built in two steps.\n",
    "\n",
    "First, we compute a linear combination of the input features:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}^T \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "Next, we apply the sigmoid function to this value:\n",
    "\n",
    "$$\n",
    "h_{\\mathbf{w}, b}(\\mathbf{x}) = g(z) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}\n",
    "$$\n",
    "\n",
    "This equation defines the **logistic regression model**.\n",
    "\n",
    "### **Interpretation of the Output**\n",
    "\n",
    "The output of logistic regression is interpreted as a **probability**:\n",
    "\n",
    "$$\n",
    "h_{\\mathbf{w}, b}(\\mathbf{x}) = P(y = 1 \\mid \\mathbf{x})\n",
    "$$\n",
    "\n",
    "For example:\n",
    "- If the model outputs $0.7$, it predicts a **70% probability** that $y = 1$\n",
    "- Since $y$ can only be $0$ or $1$, the probability that $y = 0$ is:\n",
    "\n",
    "$$\n",
    "P(y = 0 \\mid \\mathbf{x}) = 1 - 0.7 = 0.3\n",
    "$$\n",
    "\n",
    "Although the model outputs values like $0.7$ or $0.3$, the true label is always **0 or 1**.\n",
    "\n",
    "### **Why Logistic Regression Works for Classification**\n",
    "\n",
    "- Outputs are constrained between **0 and 1**\n",
    "- Predictions have a clear probabilistic interpretation\n",
    "- The sigmoid function models uncertainty near decision boundaries\n",
    "- Forms the foundation for many real-world classification systems\n",
    "\n",
    "Logistic regression is one of the most important supervised learning algorithms and serves as a stepping stone to more advanced classification methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1051b5",
   "metadata": {},
   "source": [
    "## **Decision Boundary**\n",
    "\n",
    "In logistic regression, the **decision boundary** explains **how the model converts a probability output into a class prediction (0 or 1)**. While logistic regression outputs values between 0 and 1, the decision boundary defines **where the prediction switches from class 0 to class 1**.\n",
    "\n",
    "### **Recap: Logistic Regression Model**\n",
    "\n",
    "Logistic regression computes predictions in two steps:\n",
    "\n",
    "1. **Linear combination of features**\n",
    "$$\n",
    "z = w^T x + b\n",
    "$$\n",
    "\n",
    "2. **Apply the Sigmoid (logistic) function**\n",
    "$$\n",
    "f(x) = g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "The output $f(x)$ is interpreted as:\n",
    "$$\n",
    "f(x) = P(y = 1 \\mid x; w, b)\n",
    "$$\n",
    "\n",
    "This value represents the **probability that the label is 1 given the input features**.\n",
    "\n",
    "### **From Probability to Prediction**\n",
    "\n",
    "To make a **hard classification**, we introduce a threshold. The most common choice is $0.5$:\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } f(x) \\ge 0.5 \\\\\n",
    "0 & \\text{if } f(x) < 0.5\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Because the sigmoid function satisfies:\n",
    "$$\n",
    "g(z) \\ge 0.5 \\iff z \\ge 0\n",
    "$$\n",
    "\n",
    "This means:\n",
    "$$\n",
    "\\hat{y} = 1 \\quad \\text{when} \\quad w^T x + b \\ge 0\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = 0 \\quad \\text{when} \\quad w^T x + b < 0\n",
    "$$\n",
    "\n",
    "### **Definition of the Decision Boundary**\n",
    "\n",
    "The **decision boundary** is defined by:\n",
    "$$\n",
    "w^T x + b = 0\n",
    "$$\n",
    "\n",
    "This equation represents the set of all points where the model is **indifferent** between predicting class 0 and class 1. On one side of this boundary, predictions are 1; on the other side, predictions are 0.\n",
    "\n",
    "### **Decision Boundary with Two Features**\n",
    "\n",
    "For two input features $x_1$ and $x_2$:\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "\n",
    "The decision boundary becomes:\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + b = 0\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "$$\n",
    "w_1 = 1,\\quad w_2 = 1,\\quad b = -3\n",
    "$$\n",
    "\n",
    "Decision boundary:\n",
    "$$\n",
    "x_1 + x_2 = 3\n",
    "$$\n",
    "\n",
    "- Points where $x_1 + x_2 \\ge 3 \\implies \\hat{y} = 1$  \n",
    "- Points where $x_1 + x_2 < 3 \\implies \\hat{y} = 0$  \n",
    "\n",
    "This is a **straight line** in the $x_1$–$x_2$ plane.\n",
    "\n",
    "### **Non-linear Decision Boundaries with Polynomial Features**\n",
    "\n",
    "By introducing **polynomial features**, logistic regression can learn **non-linear decision boundaries**. For example:\n",
    "\n",
    "$$\n",
    "z = w_1 x_1^2 + w_2 x_2^2 + b\n",
    "$$\n",
    "\n",
    "Decision boundary:\n",
    "$$\n",
    "w_1 x_1^2 + w_2 x_2^2 + b = 0\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "$$\n",
    "w_1 = 1,\\quad w_2 = 1,\\quad b = -1\n",
    "$$\n",
    "\n",
    "Decision boundary:\n",
    "$$\n",
    "x_1^2 + x_2^2 = 1\n",
    "$$\n",
    "\n",
    "- Inside the circle ($x_1^2 + x_2^2 < 1$) $\\implies \\hat{y} = 0$  \n",
    "- Outside the circle ($x_1^2 + x_2^2 \\ge 1$) $\\implies \\hat{y} = 1$  \n",
    "\n",
    "Higher-order polynomial features allow logistic regression to **learn very complex boundaries**, including ellipses or intricate shapes, giving flexibility to fit real-world data.\n",
    "\n",
    "### **Key Points**\n",
    "\n",
    "- Decision boundary separates predicted classes  \n",
    "- Linear logistic regression $\\implies$ linear boundaries  \n",
    "- Polynomial features $\\implies$ non-linear boundaries  \n",
    "- Defined by $w^T x + b = 0$ (or polynomial equivalent)  \n",
    "- Critical for interpreting logistic regression predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c0a9a",
   "metadata": {},
   "source": [
    "## **Cost Function for Logistic Regression**\n",
    "\n",
    "In logistic regression, the **cost function** measures how well a particular set of parameters $(w, b)$ fits the training data. Choosing an appropriate cost function is crucial because it guides the optimization process to find the best parameters.\n",
    "\n",
    "### **Why Squared Error Is Not Ideal**\n",
    "\n",
    "For linear regression, the squared error is used:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "However, using this cost function for logistic regression is problematic:\n",
    "\n",
    "- The hypothesis in logistic regression is \n",
    "$$\n",
    "f(x) = g(w^T x + b) = \\frac{1}{1 + e^{-z}}, \\quad z = w^T x + b\n",
    "$$\n",
    "- Plugging this into the squared error cost results in a **non-convex function**  \n",
    "- Non-convexity means **multiple local minima**, making gradient descent unreliable  \n",
    "\n",
    "Hence, a **different cost function** is needed to ensure convergence.\n",
    "\n",
    "### **Loss Function for a Single Training Example**\n",
    "\n",
    "Logistic regression uses a **log loss** (also called cross-entropy loss) for a single example:\n",
    "\n",
    "$$\n",
    "\\text{Loss}(f(x), y) =\n",
    "\\begin{cases} \n",
    "-\\log(f(x)) & \\text{if } y = 1 \\\\[2mm]\n",
    "-\\log(1 - f(x)) & \\text{if } y = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- If the model predicts close to the true label, the loss is small  \n",
    "- If the prediction is far from the true label, the loss increases sharply  \n",
    "\n",
    "This **penalizes confident but wrong predictions** heavily, making the algorithm learn effectively.\n",
    "\n",
    "### **Simplified Single-Equation Loss Function**\n",
    "\n",
    "Since $y \\in \\{0, 1\\}$, the two cases can be combined into a single formula:\n",
    "\n",
    "$$\n",
    "\\text{Loss}(f(x), y) = - \\Big[ y \\log(f(x)) + (1 - y) \\log(1 - f(x)) \\Big]\n",
    "$$\n",
    "\n",
    "- When $y = 1$: $- [1 \\cdot \\log(f(x)) + 0 \\cdot \\log(1 - f(x))] = -\\log(f(x))$  \n",
    "- When $y = 0$: $- [0 \\cdot \\log(f(x)) + 1 \\cdot \\log(1 - f(x))] = -\\log(1 - f(x))$  \n",
    "\n",
    "This compact form is convenient for implementation.\n",
    "\n",
    "### **Cost Function for the Entire Training Set**\n",
    "\n",
    "The cost over $m$ training examples is the **average loss**:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} \\text{Loss}(f(x^{(i)}), y^{(i)})\n",
    "$$\n",
    "\n",
    "Substituting the simplified loss function:\n",
    "\n",
    "$$\n",
    "J(w, b) = - \\frac{1}{m} \\sum_{i=1}^{m} \\Big[ y^{(i)} \\log(f(x^{(i)})) + (1 - y^{(i)}) \\log(1 - f(x^{(i)})) \\Big]\n",
    "$$\n",
    "\n",
    "- This is the standard **logistic regression cost function** used in practice  \n",
    "- It is **convex**, ensuring gradient descent converges to the global minimum  \n",
    "\n",
    "### **Intuition Behind the Cost Function**\n",
    "\n",
    "- **y = 1**: if $f(x)$ is close to 1, loss is near 0; if $f(x)$ is close to 0, loss is very high  \n",
    "- **y = 0**: if $f(x)$ is close to 0, loss is near 0; if $f(x)$ is close to 1, loss is very high  \n",
    "\n",
    "Graphically, this creates a **smooth, convex surface** for $J(w, b)$, unlike the wiggly, non-convex surface of squared error for logistic regression.\n",
    "\n",
    "### **Statistical Rationale**\n",
    "\n",
    "This cost function can be derived from **maximum likelihood estimation (MLE)**:\n",
    "\n",
    "- Logistic regression predicts probabilities $P(y = 1 \\mid x; w, b)$  \n",
    "- The likelihood of the observed data is maximized by minimizing this cost function  \n",
    "- MLE justifies the choice of **logarithmic loss** from a statistical perspective  \n",
    "\n",
    "### **Key Points**\n",
    "\n",
    "- Squared error is not suitable for logistic regression due to non-convexity  \n",
    "- The logistic loss (cross-entropy) ensures **convexity** and reliable gradient descent  \n",
    "- Single-example loss:\n",
    "$$\n",
    "\\text{Loss}(f(x), y) = - \\big[y \\log(f(x)) + (1-y)\\log(1-f(x)) \\big]\n",
    "$$\n",
    "- Overall cost function:\n",
    "$$\n",
    "J(w, b) = - \\frac{1}{m} \\sum_{i=1}^{m} \\big[ y^{(i)} \\log(f(x^{(i)})) + (1 - y^{(i)}) \\log(1 - f(x^{(i)})) \\big]\n",
    "$$\n",
    "- Provides strong penalties for wrong predictions and small penalties for correct ones  \n",
    "- Forms the foundation for gradient descent optimization in logistic regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6676f",
   "metadata": {},
   "source": [
    "## **Gradient Descent Implementation for Logistic Regression**\n",
    "\n",
    "To train a logistic regression model, we aim to find parameters $(w, b)$ that **minimize the cost function**:\n",
    "\n",
    "$$\n",
    "J(w, b) = - \\frac{1}{m} \\sum_{i=1}^{m} \\Big[ y^{(i)} \\log(f(x^{(i)})) + (1 - y^{(i)}) \\log(1 - f(x^{(i)})) \\Big]\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "f(x^{(i)}) = g(z^{(i)}) = \\frac{1}{1 + e^{-z^{(i)}}}, \\quad z^{(i)} = w^T x^{(i)} + b\n",
    "$$\n",
    "\n",
    "is the logistic regression hypothesis using the Sigmoid function.\n",
    "\n",
    "### **Gradient Descent Update Rules**\n",
    "\n",
    "Gradient descent iteratively updates the parameters in the direction that **reduces the cost function**:\n",
    "\n",
    "- For each weight $w_j$:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial J(w, b)}{\\partial w_j} \n",
    "$$\n",
    "\n",
    "- For the bias term $b$:\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "### **Gradients**\n",
    "\n",
    "The partial derivatives of the cost function are:\n",
    "\n",
    "- With respect to $w_j$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w, b)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\Big( f(x^{(i)}) - y^{(i)} \\Big) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "- With respect to $b$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w, b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\Big( f(x^{(i)}) - y^{(i)} \\Big)\n",
    "$$\n",
    "\n",
    "Here $x_j^{(i)}$ is the $j^{th}$ feature of the $i^{th}$ training example.\n",
    "\n",
    "### **Simultaneous Updates**\n",
    "\n",
    "All parameters $(w_1, w_2, ..., w_n, b)$ should be updated **simultaneously**:\n",
    "\n",
    "1. Compute all gradients using current parameters  \n",
    "2. Update all parameters using the gradients at the same time  \n",
    "\n",
    "This prevents interference between updates.\n",
    "\n",
    "### **Vectorized Implementation**\n",
    "\n",
    "The gradient descent updates can also be written in **vectorized form** for efficiency:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} - \\alpha \\frac{1}{m} X^T (f(X) - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the $m \\times n$ feature matrix  \n",
    "- $\\mathbf{w}$ is the $n \\times 1$ weight vector  \n",
    "- $f(X)$ is the vector of predictions for all training examples  \n",
    "\n",
    "Vectorization significantly speeds up computation compared to looping over each training example.\n",
    "\n",
    "### **Feature Scaling**\n",
    "\n",
    "- Feature scaling is recommended to **speed up gradient descent**  \n",
    "- Scale each feature to a similar range (e.g., $[-1, 1]$)  \n",
    "- Helps ensure all weights converge efficiently\n",
    "\n",
    "### **Algorithm Summary**\n",
    "\n",
    "1. Initialize $w_j = 0$ and $b = 0$  \n",
    "2. Repeat until convergence:\n",
    "   - Compute predictions $f(x^{(i)})$ using Sigmoid\n",
    "   - Compute gradients $\\frac{\\partial J}{\\partial w_j}$ and $\\frac{\\partial J}{\\partial b}$\n",
    "   - Update parameters using the learning rate $\\alpha$\n",
    "3. Return optimized parameters $(w, b)$\n",
    "\n",
    "### **Key Points**\n",
    "\n",
    "- Gradient descent for logistic regression is similar in form to linear regression  \n",
    "- The difference lies in using the **Sigmoid function** as the hypothesis  \n",
    "- Convex cost function ensures convergence to a **global minimum**  \n",
    "- Feature scaling improves convergence speed  \n",
    "- Vectorized implementations are more efficient for large datasets  \n",
    "\n",
    "This forms the foundation for training logistic regression models on real-world classification problems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6cacc",
   "metadata": {},
   "source": [
    "## **Overfitting**\n",
    "\n",
    "Overfitting is a common problem in machine learning where a model performs **extremely well on training data** but **fails to generalize to new, unseen examples**. It occurs when a model learns not only the underlying patterns but also the **noise or random fluctuations** in the training data. Understanding overfitting requires contrasting it with underfitting and exploring the concepts of **bias, variance, and generalization**.\n",
    "\n",
    "### **Underfitting vs Overfitting**\n",
    "\n",
    "1. **Underfitting (High Bias):**  \n",
    "   - Occurs when a model is too simple to capture the underlying structure of the data.  \n",
    "   - Example: Predicting housing prices using a straight line when the actual relationship is nonlinear.  \n",
    "   - The model performs poorly on both the training set and new data.  \n",
    "   - Also called **high bias**, because the model has a strong assumption about the data (e.g., \"prices are linear with size\") that prevents it from fitting the data well.\n",
    "\n",
    "2. **Overfitting (High Variance):**  \n",
    "   - Occurs when a model is too complex and tries to fit every training example perfectly, including noise.  \n",
    "   - Example: Fitting a fourth-order polynomial to only five house price examples. The model passes through all points but creates a wiggly curve that makes poor predictions on new houses.  \n",
    "   - The model performs very well on training data but **generalizes poorly**.  \n",
    "   - Also called **high variance**, because small changes in the training data can lead to large changes in the model.\n",
    "\n",
    "3. **Just Right Model:**  \n",
    "   - A model that **balances bias and variance**, fits the training data well, and generalizes to new examples.  \n",
    "   - Example: A quadratic model might capture the curvature in house prices without being overly complex.\n",
    "\n",
    "This relationship is often illustrated with the **Goldilocks principle**:  \n",
    "- Too simple → underfit  \n",
    "- Too complex → overfit  \n",
    "- Just right → optimal balance\n",
    "\n",
    "### **Overfitting in Regression and Classification**\n",
    "\n",
    "#### **Regression Example**\n",
    "- Features: House size, polynomial terms ($x$, $x^2$, $x^3$, $x^4$).  \n",
    "- **High-order polynomial** → overfit → perfect training fit but unrealistic predictions.  \n",
    "- **Quadratic polynomial** → just right → reasonable fit, good generalization.\n",
    "\n",
    "#### **Classification Example**\n",
    "- Features: Tumor size ($x_1$) and age ($x_2$).  \n",
    "- Logistic regression:\n",
    "  - Simple model: Straight decision boundary → underfit.  \n",
    "  - Quadratic features: Elliptical decision boundary → just right.  \n",
    "  - High-order polynomial: Complex contoured boundary → overfit, poor generalization.\n",
    "\n",
    "### **Causes of Overfitting**\n",
    "1. **Too many features** relative to the number of training examples.  \n",
    "2. **Highly flexible models** (e.g., high-degree polynomials).  \n",
    "3. **Noisy training data** with random fluctuations.\n",
    "\n",
    "### **Detecting Overfitting**\n",
    "- Large gap between **training accuracy** and **validation/test accuracy**.\n",
    "- Visual inspection: Overly complex curves in regression, twisted decision boundaries in classification.\n",
    "\n",
    "### **Techniques to Reduce Overfitting**\n",
    "\n",
    "1. **Collect More Training Data**\n",
    "   - Increasing the number of examples can reduce the variance of the model.  \n",
    "   - More data allows high-complexity models to generalize better.  \n",
    "   - Limitation: Not always feasible.\n",
    "\n",
    "2. **Feature Selection**\n",
    "   - Reduce the number of input features to **simplify the model**.  \n",
    "   - Example: Select only the most relevant features for predicting house prices (size, bedrooms, age).  \n",
    "   - Benefit: Reduces the chance of overfitting.  \n",
    "   - Limitation: Discards some information; optimal feature selection is often automated.\n",
    "\n",
    "3. **Regularization**\n",
    "   - Penalizes large parameter values to **limit model complexity**.  \n",
    "   - Instead of removing features, it **shrinks their weights**:  \n",
    "     - $L_2$ (Ridge) regularization: $\\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2$  \n",
    "     - $L_1$ (Lasso) regularization: $\\frac{\\lambda}{m} \\sum_{j=1}^{n} |w_j|$  \n",
    "   - Prevents features from dominating the prediction and reduces overfitting.  \n",
    "   - Typically, we regularize $w_1, w_2, ..., w_n$, but not the bias term $b$.  \n",
    "   - Maintains the flexibility of the model while improving generalization.\n",
    "\n",
    "### **Summary of Overfitting Remedies**\n",
    "| Method                        | How it helps                                          | Limitation                                 |\n",
    "|--------------------------------|------------------------------------------------------|-------------------------------------------|\n",
    "| **More Training Data**         | Reduces variance, smoothens model                   | Not always available                       |\n",
    "| **Feature Selection**          | Reduces model complexity                             | May discard useful information             |\n",
    "| **Regularization**             | Penalizes large weights, reduces overfitting        | Requires choosing a regularization factor |\n",
    "\n",
    "### **Key Concepts Recap**\n",
    "- **Bias:** Model’s inability to capture data patterns → underfitting.  \n",
    "- **Variance:** Model’s sensitivity to training data → overfitting.  \n",
    "- **Generalization:** Ability of the model to perform well on unseen data.  \n",
    "- **Just Right Model:** Achieves balance between bias and variance.  \n",
    "\n",
    "By carefully managing model complexity, selecting appropriate features, and using regularization, you can **prevent overfitting** and train models that generalize well to real-world data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbac8bd",
   "metadata": {},
   "source": [
    "## **Regularization in Machine Learning**\n",
    "\n",
    "Regularization is a key technique to **reduce overfitting** in machine learning models. It works by **penalizing large values of model parameters** (weights), effectively simplifying the model while retaining all features. Regularization is used in both **linear regression** and **logistic regression** and is particularly useful when the model has **many features** relative to the number of training examples.\n",
    "\n",
    "### **Intuition Behind Regularization**\n",
    "\n",
    "1. **Problem Scenario**:  \n",
    "   - Consider fitting a high-degree polynomial (e.g., fourth-order) to a small dataset of housing prices.  \n",
    "   - Without any constraints, the model might produce a **highly wiggly curve** that fits the training data perfectly but **generalizes poorly**.\n",
    "\n",
    "2. **Idea**:  \n",
    "   - If we could **force the parameters of higher-order terms** (like $w_3$ and $w_4$) to be very small, the model would behave more like a **simpler quadratic curve**.  \n",
    "   - Regularization achieves this by **adding a penalty term** to the cost function that increases as the parameters grow large.\n",
    "\n",
    "### **Modified Cost Function with Regularization**\n",
    "\n",
    "For **linear regression**, the regularized cost function becomes:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^m \\left(f(x^{(i)}) - y^{(i)}\\right)^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^n w_j^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $f(x) = w \\cdot x + b$ is the predicted output.  \n",
    "- $m$ = number of training examples.  \n",
    "- $n$ = number of features.  \n",
    "- $w_j$ = parameter for feature $j$.  \n",
    "- $\\lambda$ = **regularization parameter** controlling the trade-off between fitting the data and keeping parameters small.  \n",
    "- The bias term $b$ is typically **not regularized**.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- **First Term:** Minimizes the standard cost (mean squared error for linear regression).  \n",
    "- **Second Term (Regularization Term):** Penalizes large weights to prevent overfitting.  \n",
    "- **Choosing $\\lambda$:**  \n",
    "  - $\\lambda = 0$ → no regularization → may overfit.  \n",
    "  - Very large $\\lambda$ → heavy regularization → underfitting (weights shrink toward $0$).  \n",
    "  - Moderate $\\lambda$ → balances fitting the data and keeping the model simple.\n",
    "\n",
    "### **Gradient Descent with Regularization**\n",
    "\n",
    "To implement regularized gradient descent, the update rules change slightly:\n",
    "\n",
    "**Linear Regression Updates:**\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\left[ \\frac{1}{m} \\sum_{i=1}^m \\left(f(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} + \\frac{\\lambda}{m} w_j \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left(f(x^{(i)}) - y^{(i)}\\right)\n",
    "$$\n",
    "\n",
    "- The extra term $\\frac{\\lambda}{m} w_j$ **shrinks $w_j$ slightly** on each iteration.  \n",
    "- Intuition: $w_j \\times \\left(1 - \\alpha \\frac{\\lambda}{m}\\right)$ → gradually reduces weight magnitude.  \n",
    "- The update for $b$ **remains unchanged**.\n",
    "\n",
    "**Logistic Regression Updates:**\n",
    "\n",
    "- Regularization is applied in the **same way**, with the only difference being the function $f(x)$:  \n",
    "\n",
    "$$\n",
    "f(x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad z = w \\cdot x + b\n",
    "$$  \n",
    "\n",
    "- The derivative update for $w_j$ includes the regularization term $\\frac{\\lambda}{m} w_j$, but $b$ is not regularized.\n",
    "\n",
    "### **Effects of Regularization**\n",
    "\n",
    "| $\\lambda$ Value      | Effect on Model |\n",
    "|---------------------|----------------|\n",
    "| 0                   | No regularization → may overfit. |\n",
    "| Moderate            | Balances fit and simplicity → reduces overfitting, improves generalization. |\n",
    "| Very Large          | Heavy regularization → underfits, weights approach $0$. |\n",
    "\n",
    "- By keeping weights small, regularization **smooths the model**, preventing extreme fluctuations that cause overfitting.  \n",
    "- It allows us to **use all features** without the model relying too heavily on any single one.\n",
    "\n",
    "### **Key Concepts**\n",
    "\n",
    "1. **Regularization Parameter ($\\lambda$)**: Controls the trade-off between **training error** and **model complexity**.  \n",
    "2. **Weight Shrinking**: Each weight $w_j$ is gradually reduced during gradient descent, preventing overfitting.  \n",
    "3. **Bias Term ($b$)**: Typically not regularized. Regularizing $b$ has minimal impact.  \n",
    "4. **General Principle**: Regularization makes complex models behave more like simpler models without discarding features.\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "Regularization is a **powerful tool** to prevent overfitting:\n",
    "\n",
    "1. Modify the cost function to include a **penalty for large weights**.  \n",
    "2. Adjust **gradient descent updates** to shrink weights slightly each iteration.  \n",
    "3. Choose a **$\\lambda$ value** that balances fit and simplicity.  \n",
    "4. Apply the same idea to **both linear and logistic regression**.  \n",
    "\n",
    "By implementing regularization, even **high-dimensional models** (many features) can generalize well to unseen data, making this technique essential for practical machine learning applications.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
