{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GkxmPtUDoeo"
      },
      "source": [
        "<h2 style=\"text-align: center;\"><strong>Segment 2: Integrals</strong></h2>\n",
        "\n",
        "* Integration in Machine Learning\n",
        "* Numeric Integration with Python\n",
        "* Binary Classification\n",
        "* The Confusion Matrix\n",
        "* The Receiver-Operating Characteristic (ROC) Curve \n",
        "* Area Under the ROC Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Integration in Machine Learning**\n",
        "\n",
        "Integration is a fundamental mathematical tool that measures the **total accumulation of a quantity** over a continuous domain. While derivatives measure instantaneous change, integrals measure **aggregate effects**, which is crucial in probabilistic reasoning, expected values, continuous loss computation, and more in ML.\n",
        "\n",
        "### **Basic Integration Rules**\n",
        "\n",
        "1. **Power Rule**  \n",
        "$$\n",
        "\\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C, \\quad n \\neq -1\n",
        "$$\n",
        "\n",
        "2. **Constant Multiple Rule**  \n",
        "$$\n",
        "\\int k \\cdot f(x) \\, dx = k \\int f(x) \\, dx + C\n",
        "$$\n",
        "\n",
        "3. **Sum Rule**  \n",
        "$$\n",
        "\\int [f(x) + g(x)] \\, dx = \\int f(x) \\, dx + \\int g(x) \\, dx + C\n",
        "$$\n",
        "\n",
        "4. **Definite Integral** (area under the curve from $a$ to $b$)  \n",
        "$$\n",
        "\\int_a^b f(x) \\, dx = F(b) - F(a)\n",
        "$$\n",
        "where $F(x)$ is the antiderivative of $f(x)$. Note that the **constant of integration $C$ cancels** in definite integrals.\n",
        "\n",
        "### **Applications of Integration in Machine Learning**\n",
        "\n",
        "**1. Expected Value and Probability**\n",
        "\n",
        "For a continuous random variable $X$ with probability density function $p(x)$:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[f(X)] = \\int_{-\\infty}^{\\infty} f(x) \\, p(x) \\, dx\n",
        "$$\n",
        "\n",
        "- Used in Bayesian inference, probabilistic neural networks, and expectation-maximization algorithms.  \n",
        "- **Example:** If $X \\sim \\text{Uniform}[0,1]$, then\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[X] = \\int_0^1 x \\, dx = \\left[ \\frac{x^2}{2} + C \\right]_0^1 = \\frac{1}{2}\n",
        "$$\n",
        "\n",
        "**2. Definite Integrals for Continuous Loss**\n",
        "\n",
        "Some loss functions may be defined continuously over the input domain:\n",
        "\n",
        "$$\n",
        "L = \\int (f_\\theta(x) - y(x))^2 \\, dx\n",
        "$$\n",
        "\n",
        "- Measures **total error** across all inputs.  \n",
        "- Often approximated numerically in high-dimensional spaces.\n",
        "\n",
        "**3. Probability Distribution Normalization**\n",
        "\n",
        "For a probability density function $p(x)$:\n",
        "\n",
        "$$\n",
        "\\int_{-\\infty}^{\\infty} p(x) \\, dx = 1\n",
        "$$\n",
        "\n",
        "- Ensures total probability sums to 1, as used in Gaussian distributions, Bayesian priors, and generative models.\n",
        "\n",
        "**4. Cumulative Distribution Function (CDF)**\n",
        "\n",
        "The CDF is the integral of the PDF:\n",
        "\n",
        "$$\n",
        "F(x) = \\int_{-\\infty}^{x} p(t) \\, dt\n",
        "$$\n",
        "\n",
        "- Represents the probability that a variable is less than or equal to $x$.  \n",
        "- Useful in sampling and thresholding decisions.\n",
        "\n",
        "**5. Continuous-Time Models**\n",
        "\n",
        "Neural ODEs and other continuous-time models use integration to evolve states:\n",
        "\n",
        "$$\n",
        "h(t_1) = h(t_0) + \\int_{t_0}^{t_1} f(h(t), t, \\theta) \\, dt\n",
        "$$\n",
        "\n",
        "- Accumulates infinitesimal changes in the hidden state over time.\n",
        "\n",
        "**6. Area Under the Curve (AUC)**\n",
        "\n",
        "In classification, integration is used to compute **Area Under the ROC Curve (AUC)**:\n",
        "\n",
        "$$\n",
        "\\text{AUC} = \\int_0^1 TPR(FPR) \\, dFPR\n",
        "$$\n",
        "\n",
        "- Quantifies overall model performance across thresholds.\n",
        "\n",
        "##### **Example: Area Under a Polynomial Curve**\n",
        "\n",
        "Compute the area under:\n",
        "\n",
        "$$\n",
        "f(x) = 3x^2 + 2x + 1, \\quad x \\in [0,2]\n",
        "$$\n",
        "\n",
        "**Solution:**\n",
        "\n",
        "1. Apply the **sum rule** and **constant multiple rule** with the **power rule**:\n",
        "\n",
        "$$\n",
        "\\int (3x^2 + 2x + 1) dx = \\int 3x^2 \\, dx + \\int 2x \\, dx + \\int 1 \\, dx\n",
        "$$\n",
        "\n",
        "- $\\int 3x^2 \\, dx = 3 \\cdot \\frac{x^3}{3} + C = x^3 + C$  \n",
        "- $\\int 2x \\, dx = 2 \\cdot \\frac{x^2}{2} + C = x^2 + C$  \n",
        "- $\\int 1 \\, dx = x + C$\n",
        "\n",
        "Combine:\n",
        "\n",
        "$$\n",
        "\\int (3x^2 + 2x + 1) dx = x^3 + x^2 + x + C\n",
        "$$\n",
        "\n",
        "2. Evaluate the **definite integral** from 0 to 2:\n",
        "\n",
        "$$\n",
        "\\int_0^2 (3x^2 + 2x + 1) dx = \\left[ x^3 + x^2 + x \\right]_0^2 = (8 + 4 + 2) - (0 + 0 + 0) = 14\n",
        "$$\n",
        "\n",
        "**Interpretation:** The total accumulated quantity under the curve is **14**.\n",
        "\n",
        "##### **Key Concepts**\n",
        "\n",
        "- Integration **accumulates contributions** over a domain.  \n",
        "- Essential in **expected value computation, probabilistic modeling, continuous loss, and continuous-time dynamics**.  \n",
        "- Definite integrals correspond to **area under curves**, interpreted as total probability, total loss, or accumulated contribution.  \n",
        "- **The constant of integration $C$** appears in indefinite integrals but cancels in definite integrals.  \n",
        "- Numerical methods (trapezoidal, Monte Carlo, `scipy.integrate.quad`) are used when analytical solutions are not feasible.\n",
        "\n",
        "##### **Summary**\n",
        "\n",
        "> Integration in ML is a cornerstone for **probability, expectation, continuous loss, state evolution, and performance metrics**. Mastery of integration rules and applications bridges **calculus concepts** with practical ML tasks.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Numeric Integration with Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Computing Definite Integrals Numerically with* `scipy.integrate.quad`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "7RNXGSU_Doep"
      },
      "outputs": [],
      "source": [
        "from scipy.integrate import quad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gcCUd4QDoep"
      },
      "source": [
        "$$ \\int_1^2 \\frac{x}{2} dx = \\frac{3}{4} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "lQSGdUlIDoep"
      },
      "outputs": [],
      "source": [
        "def g(x):\n",
        "    return x/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ifu7x0iRDoep",
        "outputId": "cc0b40e2-6a61-4662-e835-16ddbbc3c378"
      },
      "outputs": [],
      "source": [
        "area, error = quad(g, 1, 2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "area"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`quad` *returns an estimated error due to numerical approximation, but it is typically so small that it can be ignored.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8.326672684688674e-15"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Binary Classification**\n",
        "\n",
        "**Binary classification** is a supervised learning task where the goal is to assign input data to **one of two classes**, typically labeled 0 and 1.\n",
        "\n",
        "##### **Problem Setup**\n",
        "\n",
        "Given a dataset of $n$ examples:\n",
        "\n",
        "$$\n",
        "\\{(x^{(i)}, y^{(i)})\\}_{i=1}^n, \\quad y^{(i)} \\in \\{0, 1\\}\n",
        "$$\n",
        "\n",
        "The model $f_\\theta(x)$ predicts:\n",
        "\n",
        "$$\n",
        "\\hat{y}^{(i)} = f_\\theta(x^{(i)}) \\in [0, 1]\n",
        "$$\n",
        "\n",
        "Here, $\\hat{y}^{(i)}$ is interpreted as the **probability of belonging to class 1**. The final class can be assigned using a threshold, e.g., 0.5.\n",
        "\n",
        "##### **Model Example: Logistic Regression**\n",
        "\n",
        "A simple linear model with sigmoid activation:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad z = \\theta^T x\n",
        "$$\n",
        "\n",
        "- Maps any real-valued input to a probability.  \n",
        "- Suitable for modeling **linear decision boundaries**.\n",
        "\n",
        "##### **Cost Function**\n",
        "\n",
        "The **binary cross-entropy (log loss)** measures the difference between predicted probabilities and true labels:\n",
        "\n",
        "$$\n",
        "J(\\theta) = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)}) \\right]\n",
        "$$\n",
        "\n",
        "- Penalizes wrong predictions more heavily when they are confident.  \n",
        "- Gradients of $J(\\theta)$ are used to **update parameters via gradient descent**.\n",
        "\n",
        "##### **Key Concepts**\n",
        "\n",
        "1. **Decision Boundary:** Separates the two classes in feature space.  \n",
        "2. **Probability Interpretation:** Outputs $\\hat{y} \\in [0,1]$; thresholding gives class labels.  \n",
        "3. **Gradient-Based Training:** Gradients guide updates to reduce the cost function.  \n",
        "4. **Evaluation Metrics:** Accuracy, precision, recall, F1-score, ROC-AUC.\n",
        "\n",
        "##### **Conceptual Understanding**\n",
        "\n",
        "- Illustrates **how the model predicts probabilities**.  \n",
        "- Shows **how the cost function guides learning**.  \n",
        "- Explains **how predictions are converted to binary decisions**.  \n",
        "- Demonstrates **how gradients drive parameter updates**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **The Confusion Matrix**\n",
        "\n",
        "A **confusion matrix** is a table used to evaluate the performance of a classification model by comparing **predicted labels** against **true labels**.\n",
        "\n",
        "For binary classification:\n",
        "\n",
        "|                | Predicted 0 | Predicted 1 |\n",
        "|----------------|-------------|-------------|\n",
        "| **Actual 0**   | True Negative (TN) | False Positive (FP) |\n",
        "| **Actual 1**   | False Negative (FN) | True Positive (TP) |\n",
        "\n",
        "##### **Key Metrics Derived**\n",
        "\n",
        "- **Accuracy:** Overall correctness  \n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "- **Precision:** Correctness of positive predictions  \n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "- **Recall / Sensitivity / True Positive Rate (TPR):** Ability to correctly identify positives  \n",
        "\n",
        "$$\n",
        "\\text{TPR} = \\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "- **False Positive Rate (FPR):** Proportion of negatives misclassified as positive  \n",
        "\n",
        "$$\n",
        "\\text{FPR} = \\frac{FP}{FP + TN}\n",
        "$$\n",
        "\n",
        "- **F1-Score:** Harmonic mean of precision and recall  \n",
        "\n",
        "$$\n",
        "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "##### **Conceptual Understanding**\n",
        "\n",
        "- The confusion matrix shows **where the model makes errors**.  \n",
        "- **TPR** indicates how well the model detects positive cases.  \n",
        "- **FPR** indicates how often negative cases are incorrectly labeled as positive.  \n",
        "- Metrics like precision, recall, and F1-score are particularly important for **imbalanced datasets**, where accuracy alone can be misleading.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **The Receiver-Operating Characteristic (ROC) Curve**\n",
        "\n",
        "The **ROC curve** is a graphical tool to evaluate the performance of a binary classifier across **all possible thresholds**. It plots the trade-off between **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
        "\n",
        "##### **Axes of the ROC Curve**\n",
        "\n",
        "- **X-axis:** False Positive Rate (FPR)  \n",
        "\n",
        "$$\n",
        "\\text{FPR} = \\frac{FP}{FP + TN}\n",
        "$$\n",
        "\n",
        "- **Y-axis:** True Positive Rate (TPR = Recall)  \n",
        "\n",
        "$$\n",
        "\\text{TPR} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "Each point on the curve corresponds to a **different classification threshold**. As the threshold changes, the TPR and FPR change, producing the curve.\n",
        "\n",
        "##### **Key Concepts**\n",
        "\n",
        "1. **Threshold Variation:** By lowering the threshold, more examples are predicted as positive → increases TPR but also increases FPR.  \n",
        "2. **Area Under the Curve (AUC):**  \n",
        "   - $\\text{AUC} = 1$ → perfect classifier  \n",
        "   - $\\text{AUC} = 0.5$ → random guessing  \n",
        "   - Higher AUC indicates better overall performance.  \n",
        "3. **Comparison Tool:** ROC curves are useful for comparing classifiers, especially with **imbalanced datasets**.  \n",
        "4. **Connection to Confusion Matrix:** Each point on the ROC curve can be derived from the **confusion matrix** at a specific threshold, using TPR and FPR.\n",
        "\n",
        "##### **Conceptual Understanding**\n",
        "\n",
        "- ROC curves show the **trade-off between sensitivity and specificity**.  \n",
        "- They help **select an optimal threshold** depending on the cost of false positives vs false negatives.  \n",
        "- A classifier closer to the **top-left corner** of the ROC space is generally better.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAmdDoskDoeq"
      },
      "source": [
        "## **Area Under the ROC Curve**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_wFvlHEDoeq"
      },
      "source": [
        "When we have discrete $(x, y)$ coordinates instead of a continuous function, we can compute the area under the curve using scikit-learn's `auc()` method. This method employs a numerical approach based on the [trapezoidal rule](https://en.wikipedia.org/wiki/Trapezoidal_rule) to approximate the integral of the curve defined by the coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "_X0uiqKLDoeq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XilqUs--Doer"
      },
      "source": [
        "The $(x, y)$ coordinates of the ROC curve are:\n",
        "\n",
        "* $(0, 0)$\n",
        "* $(0, 0.5)$\n",
        "* $(0.5, 0.5)$\n",
        "* $(0.5, 1)$\n",
        "* $(1, 1)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "yu9wg0aoDoer"
      },
      "outputs": [],
      "source": [
        "FPR = [0, 0, 0.5, 0.5, 1]\n",
        "TPR = [0, 0.5, 0.5, 1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwnqDGueDoer",
        "outputId": "9a1ba72e-0bc6-41e9-9d2f-ee2583fffa3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "auc(FPR, TPR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "4-calculus-ii.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
