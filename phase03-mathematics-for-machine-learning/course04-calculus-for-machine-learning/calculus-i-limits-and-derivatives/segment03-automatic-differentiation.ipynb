{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez8pKj_lcxbG"
      },
      "source": [
        "<h2 style=\"text-align: center;\"><strong>Segment 3: Automatic Differentiation</strong></h2>\n",
        "\n",
        "* AutoDiff with PyTorch and TensorFlow 2\n",
        "* Machine Learning via Differentiation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **AutoDiff with PyTorch and TensorFlow 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laWM6cgvcxbG"
      },
      "source": [
        "**PyTorch** and **TensorFlow** are the two most popular automatic differentiation libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLi7jwLEzaDB"
      },
      "source": [
        "Let's use them to calculate $dy/dx$ at $x = 5$ where: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmgeCDWycxbL"
      },
      "source": [
        "$$y = x^2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhMxT9oQcxbL"
      },
      "source": [
        "$$ \\frac{dy}{dx} = 2x = 2(5) = 10 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg-PepdncxbN"
      },
      "source": [
        "##### **AutoDiff with PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTYVufujcxbP"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor(5.0)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Contagiously track gradients through forward pass*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBLvpsCWcxbQ",
        "outputId": "6f50d93d-6e9d-40b2-f66a-2190b2b1e445"
      },
      "outputs": [],
      "source": [
        "x.requires_grad_() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zper8WoVcxbR"
      },
      "outputs": [],
      "source": [
        "y = x**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Using autodiff*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Pbdg7zVcxbS"
      },
      "outputs": [],
      "source": [
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbV3BecacxbT",
        "outputId": "9d3f5270-a22c-40cc-dede-e51d75209c6a"
      },
      "outputs": [],
      "source": [
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyB3FghkcxbG"
      },
      "source": [
        "##### **Autodiff with TensorFlow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7mQuelZcxbJ"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(5.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Track forward pass*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-WeFuBfcxbK"
      },
      "outputs": [],
      "source": [
        "with tf.GradientTape() as t:\n",
        "    t.watch(x) \n",
        "    y = x**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Using autodiff*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Txjp5NBNcxbL",
        "outputId": "00862329-4eae-4129-f6e7-971c718039f5"
      },
      "outputs": [],
      "source": [
        "t.gradient(y, x) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS--vwVWzaDD"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l599s7vacxbV"
      },
      "source": [
        "## **Machine Learning via Differentiation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*We use PyTorch’s automatic differentiation library to fit a straight line to a very small dataset containing only a few data points. This time, the regression problem is solved using auto-diff, rather than the* **Moore–Penrose pseudoinverse** *approach covered earlier in* **Phase 3, Course 3: Linear Algebra for Machine Learning.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$x$ *represents the **dosage levels of a drug** administered to patients in a study on Alzheimer's disease.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7.])\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$y$ *represents the patients forgetfulness scores corresponding to each drug dosage.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = torch.tensor([1.86, 1.31, .62, .33, .09, -.67, -1.23, -1.37]) \n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Plot data points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "plt.title(\"Clinical Trial\",fontweight=\"bold\")\n",
        "plt.xlabel(\"Drug dosage (mL)\")\n",
        "plt.ylabel(\"Forgetfulness\")\n",
        "ax.scatter(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*The target $y$ values are generated from the linear equation $y = mx + b$, which lets us know the true parameters the model should learn. In our case, the underlying line uses $m = 0.9$ and $b = 0.1$. To introduce a bit of realism, we add random, normally distributed noise to the data to simulate sampling variability. We will use PyTorch’s automatic differentiation (autodiff) to learn these parameters from the noisy observations.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "m = nn.Parameter(torch.tensor(0.9))\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b = nn.Parameter(torch.tensor(0.1))\n",
        "b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*We define a simple regression function that computes the predicted value $\\hat{y}$ for a given input $x$ using the linear model $y = mx + b$*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def regression(m,x,b):\n",
        "    return m*x + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Function to Plot the Noisy Data and the Learned Linear Model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def regression_plot(x, y, m, b):\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    ax.scatter(x, y)\n",
        "    x_min, x_max = ax.get_xlim()\n",
        "    y_min = regression(m, x_min, b).item()\n",
        "    y_max = regression(m, x_max, b).item()\n",
        "    ax.plot([x_min, x_max], [y_min, y_max])\n",
        "\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regression_plot(x,y,m,b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Machine Learning*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1**: Forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat = regression(m,x,b)\n",
        "yhat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2**: Compare $\\hat{y}$ with true $y$ to calculate cost $C$\n",
        "\n",
        "> There is a PyTorch **MSELoss** method, but let's define it outselves to see how it works. MSE cost is defined by:* $$C = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y_i}-y_i)^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse(yhat, y): \n",
        "    sigma = torch.sum((yhat - y)**2)\n",
        "    return sigma/len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "C = mse(yhat, y)\n",
        "C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3**: Use Autodiff to calculate gradient of $C$ w.r.t. parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "C.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4**: Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD([m, b], lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Let's confirm parameters have been adjusted sensibly*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regression_plot(x, y, m, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*We can repeat steps 1 and 2 to confirm cost has decreased*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "C = mse(regression(m,x,b), y)\n",
        "C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Put the 4 steps in a loop to iteratively minimize cost toward zero:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    yhat = regression(m,x,b) \n",
        "    C = mse(yhat, y) \n",
        "    \n",
        "    C.backward() \n",
        "    optimizer.step() \n",
        "    \n",
        "    print('Epoch {}, cost {}, m grad {}, b grad {}'.format(epoch, '%.3g' % C.item(), '%.3g' % m.grad.item(), '%.3g' % b.grad.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regression_plot(x, y, m, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "3-calculus-i.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
